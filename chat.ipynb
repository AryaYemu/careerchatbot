{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mb7419/career-guidance-reddit couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\DELL\\.cache\\huggingface\\datasets\\mb7419___career-guidance-reddit\\default\\0.0.0\\27a7fb49e3bb94b42587a93482c80c97dff3ed34 (last modified on Wed Feb 19 15:35:40 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'd1qr9r', 'title': 'How old were you when you knew what you wanted to do?', 'body': 'I think it s so dumb that we force 18 year olds to know what they want to do for the rest of their lives. I m 28 and I only just figured out what I wanted to do like 4 years ago.   How old were you and what did you end up doing?', 'created_utc': '2019-09-09 13:15:22', 'url': 'https://www.reddit.com/r/careerguidance/comments/d1qr9r/how_old_were_you_when_you_knew_what_you_wanted_to/', 'retrieved_on': '2020-04-04 19:48:42', 'question_content': 'how old were you when you knew what you wanted to do  i think it s so dumb that we force 18 year olds to know what they want to do for the rest of their lives  i m 28 and i only just figured out what i wanted to do like 4 years ago    how old were you and what did you end up doing ', 'dominant_topic': 0, 'dominant_topic_name': 'Career Exploration and Uncertainty', '__index_level_0__': 6462}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Career Guidance Reddit Dataset\n",
    "dataset = load_dataset(\"mb7419/career-guidance-reddit\")\n",
    "\n",
    "# Print an example\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "Successfully installed safetensors-0.5.2 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py): started\n",
      "  Building wheel for pytorch (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [6 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-install-3439g9gz\\pytorch_70059b2042fa436cab31e2880198eca7\\setup.py\", line 15, in <module>\n",
      "          raise Exception(message)\n",
      "      Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pytorch\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pytorch)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install torch\n",
    "# ! pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7bcbc3b9c64d62b0ea60bc134d022a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20669f03ad840f2b5936d97d9c97b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c620b83a03af47618f00554c31c2704c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load BERT tokenizer & model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, force_download=True)\n",
    "\n",
    "# Function to generate embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Create FAISS Index for fast search\n",
    "d = 768  # BERT embedding size\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Store embeddings & questions\n",
    "career_questions = []\n",
    "career_answers = []\n",
    "career_vectors = []\n",
    "\n",
    "for entry in dataset[\"train\"]:\n",
    "    question = entry[\"title\"]\n",
    "    answer = entry[\"body\"]\n",
    "    embedding = get_embedding(question)\n",
    "\n",
    "    career_questions.append(question)\n",
    "    career_answers.append(answer)\n",
    "    career_vectors.append(embedding)\n",
    "\n",
    "# Convert to FAISS format\n",
    "career_vectors = np.vstack(career_vectors)\n",
    "faiss_index.add(career_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Career Advice: I m currently in online learning to obtain my Medical &amp  Coding License  however  I ve always had a passion for Technology. I decided to browse online and found the following link. Everything within the program overview fascinates me and I feel like this could be my calling  however  I just want to have a solid idea of what is required or what I need to obtain a job within this career field. I ve already requested information online and I wanted to ask my fellow Redditors for their support. Any help  advice  or wisdom would be much appreciated.\n"
     ]
    }
   ],
   "source": [
    "def retrieve_career_advice(user_query):\n",
    "    user_embedding = get_embedding(user_query)\n",
    "    _, idx = faiss_index.search(user_embedding, 1)  # Find closest match\n",
    "    best_match_index = idx[0][0]\n",
    "    \n",
    "    return career_answers[best_match_index]\n",
    "\n",
    "# Example Usage\n",
    "user_input = \"What skills do I need to be a software engineer?\"\n",
    "response = retrieve_career_advice(user_input)\n",
    "print(\"Career Advice:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 992.0/992.0 kB 5.8 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'meta-llama/Llama-2-7b-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load LLaMA 2 model & tokenizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m llama_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m llama_model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Function to generate LLaMA-based responses\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2036\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2033\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2034\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2038\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2039\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2040\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2041\u001b[0m     )\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'meta-llama/Llama-2-7b-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Load LLaMA 2 model & tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "llama_model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# Function to generate LLaMA-based responses\n",
    "def generate_llama_response(user_query):\n",
    "    input_ids = llama_tokenizer.encode(user_query, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_ids = llama_model.generate(input_ids, max_length=100)\n",
    "    response = llama_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
